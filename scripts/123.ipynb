{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import openpyxl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.init as init\n",
    "from transformers import Wav2Vec2Model\n",
    "import sys\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "# model = nemo_asr.models.EncDecRNNTModel.from_pretrained(\"nvidia/stt_en_conformer_transducer_xlarge\")\n",
    "model = nemo_asr.models.EncDecRNNTModel.from_pretrained(\"nvidia/stt_en_conformer_transducer_xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = model.transcribe(paths2audio_files = [\"/home/ubuntu/elec823/clean.wav\", \"/home/ubuntu/elec823/clean2.wav\"], return_hypotheses = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = a.word_confidence\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from utils import *\n",
    "from my_loss import *\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "# model = nemo_asr.models.EncDecRNNTModel.from_pretrained(\"nvidia/stt_en_conformer_transducer_xlarge\")\n",
    "model = nemo_asr.models.EncDecRNNTModel.from_pretrained(\"nvidia/stt_en_conformer_transducer_xlarge\")\n",
    "\n",
    "MODEL = '01_mel_mono_freeze'\n",
    "\n",
    "CONSTANTS = InitializationTrain(\n",
    "    model_name=MODEL, \n",
    "    verbose=True,\n",
    "    train_data_path='/home/ubuntu/elec823/clarity_CPC1_data/clarity_data/HA_outputs/train/',\n",
    "    train_info_path='/home/ubuntu/elec823/clarity_CPC1_data/metadata/CPC1.train.json',\n",
    "    train_audiogram_path='/home/ubuntu/elec823/clarity_CPC1_data/metadata/listeners.CPC1_train.json',\n",
    "    train_listener_path='/home/ubuntu/elec823/clarity_CPC1_data/metadata/listener_data.CPC1_train.xlsx',\n",
    "    log_path='/home/ubuntu/elec823/log/',\n",
    "    save_path='/home/ubuntu/elec823/checkpoints/',\n",
    "    orig_freq=32000,\n",
    "    new_freq=16_000,\n",
    "    seed=3407,\n",
    "    device=None\n",
    "    )\n",
    "dataset = CPCdata(metadata=CONSTANTS.metadata)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    train_loader = DataLoader(dataset=[dataset[i] for i in train_idx], batch_size=3, shuffle=True, pin_memory=True,num_workers=8)\n",
    "    break\n",
    "for speech_input, info_dict in train_loader:\n",
    "    mono_path = [CONSTANTS.DATA_PATH + path + \"_mono.wav\" for path in info_dict['path']]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_path = ['/home/ubuntu/elec823/clarity_CPC1_data/clarity_data/HA_outputs/train/S09844_L0231_E013_mono.wav',\n",
    " '/home/ubuntu/elec823/clarity_CPC1_data/clarity_data/HA_outputs/train/S09928_L0219_E021_mono.wav',\n",
    " '/home/ubuntu/elec823/clarity_CPC1_data/clarity_data/HA_outputs/train/S09463_L0209_E013_mono.wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.transcribe(paths2audio_files = converted_path, return_hypotheses = True)\n",
    "# Use out[0]\n",
    "# then will be a list of 'batch' hypotheses\n",
    "# word confidence is out[0][i].word_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = [out[0][i].word_confidence for i in range(len(out[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(confidence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = confidence\n",
    "def truncate_and_pad(tensor):\n",
    "    FIXED_LENGTH = 10\n",
    "    if type(tensor) is not torch.Tensor:\n",
    "        tensor = torch.tensor(tensor)\n",
    "    current_length = tensor.size(0)\n",
    "\n",
    "    # If the tensor length is greater than the target length, truncate it\n",
    "    if current_length > FIXED_LENGTH:\n",
    "        tensor = tensor[:FIXED_LENGTH]\n",
    "    # If the tensor length is less than the target length, pad it with zeros\n",
    "    elif current_length < FIXED_LENGTH:\n",
    "        pad_size = FIXED_LENGTH - current_length\n",
    "        padding = torch.zeros(pad_size, dtype=tensor.dtype, device=tensor.device)\n",
    "        tensor = torch.cat((tensor, padding), dim=0)\n",
    "    return tensor\n",
    "\n",
    "result = list(map(truncate_and_pad, t1))\n",
    "print(result)\n",
    "print(len(result))\n",
    "print(len(result[0]))\n",
    "print(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.stack(result, dim=0)\n",
    "print(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.LSTM(input_size=10,\n",
    "                    hidden_size=256,\n",
    "                    num_layers=3,\n",
    "                    batch_first=True,\n",
    "                    bidirectional=True)\n",
    "out, (h_n, c_n) = m(a)\n",
    "print(out.shape)\n",
    "m2 = nn.Linear(512, 1)\n",
    "m2(out)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(\"nvidia/stt_en_conformer_transducer_xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence, _ = asr_model.transcribe(paths2audio_files = converted_path, return_hypotheses = True)\n",
    "confidence = [confidence[i].word_confidence for i in range(len(confidence))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confidence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = torch.stack(list(map(truncate_and_pad, confidence)), dim=0)\n",
    "print(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_lstm_predictor = nn.Sequential(\n",
    "            nn.LSTM(input_size=10,\n",
    "                    hidden_size=256,\n",
    "                    num_layers=3,\n",
    "                    batch_first=True,\n",
    "                    bidirectional=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "t1 = torch.randn(3, 10)\n",
    "t2 = bidirectional_lstm_predictor(t1)\n",
    "print(t2)\n",
    "out = bidirectional_lstm_predictor(confidence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import openpyxl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.init as init\n",
    "from transformers import Wav2Vec2Model\n",
    "import sys\n",
    "\n",
    "from utils import *\n",
    "\n",
    "class WordConfidence(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WordConfidence, self).__init__()\n",
    "        self.asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(\"nvidia/stt_en_conformer_transducer_xlarge\")\n",
    "        self.bidirectional_lstm=nn.LSTM(\n",
    "            input_size=10,\n",
    "            hidden_size=256,\n",
    "            num_layers=3,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "            )\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, speech_input, meta_data, mono_path):\n",
    "        # output of asr model is [B, word_len]\n",
    "        confidence, _ = self.asr_model.transcribe(paths2audio_files = mono_path, return_hypotheses = True)\n",
    "        confidence = [confidence[i].word_confidence for i in range(len(confidence))]\n",
    "        # padding and truncating to 10: output shape [B, 10]\n",
    "        confidence = torch.stack(list(map(self.truncate_and_pad, confidence)), dim=0)\n",
    "        lstm_out, (h_n, c_n) = self.bidirectional_lstm(confidence)\n",
    "        pred = self.predictor(lstm_out)\n",
    "        \n",
    "        return pred\n",
    "\n",
    "        \n",
    "    def truncate_and_pad(self, tensor):\n",
    "        FIXED_LENGTH = 10\n",
    "        if type(tensor) is not torch.Tensor:\n",
    "            tensor = torch.tensor(tensor)\n",
    "        current_length = tensor.size(0)\n",
    "\n",
    "        # If the tensor length is greater than the target length, truncate it\n",
    "        if current_length > FIXED_LENGTH:\n",
    "            tensor = tensor[:FIXED_LENGTH]\n",
    "        # If the tensor length is less than the target length, pad it with zeros\n",
    "        elif current_length < FIXED_LENGTH:\n",
    "            pad_size = FIXED_LENGTH - current_length\n",
    "            padding = torch.zeros(pad_size, dtype=tensor.dtype, device=tensor.device)\n",
    "            tensor = torch.cat((tensor, padding), dim=0)\n",
    "        return tensor\n",
    "model = WordConfidence()\n",
    "converted_path = ['/home/ubuntu/elec823/clarity_CPC1_data/clarity_data/HA_outputs/train/S09844_L0231_E013_mono.wav',\n",
    " '/home/ubuntu/elec823/clarity_CPC1_data/clarity_data/HA_outputs/train/S09928_L0219_E021_mono.wav',\n",
    " '/home/ubuntu/elec823/clarity_CPC1_data/clarity_data/HA_outputs/train/S09463_L0209_E013_mono.wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(1,1,converted_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tensor1 = torch.tensor([1., 2, 3.])\n",
    "tensor2 = torch.tensor([1., 2. , 3.])\n",
    "\n",
    "# Convert the shape of tensor2 to match tensor1\n",
    "tensor2_new = tensor2.view_as(tensor1)\n",
    "print(tensor2_new)\n",
    "\n",
    "print(tensor2_new.shape)  # Output: torch.Size([1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name:  None\n",
      "Warning: Model name is not specified. It's EXPECTED if you are initializing a 'models.py'. Otherwise Your model will be saved in 'checkpoints/00_TMP_MODEL' folder.\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "CONSTANTS = InitializationTrain(\n",
    "    verbose=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Tesla T4\n",
      "Model Name:  None\n",
      "Checking paths...\n",
      "Warning: Model name is not specified. It's EXPECTED if you are initializing a 'models.py'. Otherwise Your model will be saved in 'checkpoints/00_TMP_MODEL' folder.\n",
      "Folder already exists: /home/ubuntu/elec823/log/00_TMP_MODEL\n",
      "Folder already exists: /home/ubuntu/elec823/checkpoints/00_TMP_MODEL\n",
      "Folder already exists: /home/ubuntu/elec823/log/last_output/\n",
      "Seed set to: 3407\n"
     ]
    }
   ],
   "source": [
    "from data_process import CPCdataMono\n",
    "dataset = CPCdataMono(metadata=CONSTANTS.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4863"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-03-31 10:44:31 optimizers:54] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
      "[NeMo W 2023-03-31 10:44:32 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-03-31 10:44:36 mixins:170] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-03-31 10:44:36 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath:\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket1/tarred_audio_manifest.json\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket2/tarred_audio_manifest.json\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket3/tarred_audio_manifest.json\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket4/tarred_audio_manifest.json\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket5/tarred_audio_manifest.json\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket6/tarred_audio_manifest.json\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket7/tarred_audio_manifest.json\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket8/tarred_audio_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 1\n",
      "    shuffle: true\n",
      "    num_workers: 4\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20\n",
      "    min_duration: 0\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths:\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket1/audio__OP_0..8191_CL_.tar\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket2/audio__OP_0..8191_CL_.tar\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket3/audio__OP_0..8191_CL_.tar\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket4/audio__OP_0..8191_CL_.tar\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket5/audio__OP_0..8191_CL_.tar\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket6/audio__OP_0..8191_CL_.tar\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket7/audio__OP_0..8191_CL_.tar\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket8/audio__OP_0..8191_CL_.tar\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: synced_randomized\n",
      "    bucketing_batch_size:\n",
      "    - 32\n",
      "    - 32\n",
      "    - 16\n",
      "    - 16\n",
      "    - 16\n",
      "    - 16\n",
      "    - 8\n",
      "    - 8\n",
      "    \n",
      "[NeMo W 2023-03-31 10:44:36 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - /data/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 4\n",
      "    shuffle: false\n",
      "    num_workers: 4\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    min_duration: 0\n",
      "    \n",
      "[NeMo W 2023-03-31 10:44:36 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /data/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 4\n",
      "    shuffle: false\n",
      "    num_workers: 4\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-03-31 10:44:36 features:286] PADDING: 0\n",
      "[NeMo I 2023-03-31 10:44:42 rnnt_models:206] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2023-03-31 10:44:48 save_restore_connector:247] Model EncDecRNNTBPEModel was successfully restored from /home/ubuntu/.cache/huggingface/hub/models--nvidia--stt_en_conformer_transducer_xlarge/snapshots/96472b7552a5d0559a22399ea300498c5412699f/stt_en_conformer_transducer_xlarge.nemo.\n"
     ]
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(\"nvidia/stt_en_conformer_transducer_xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.transcribe(paths2audio_files = ['/home/ubuntu/elec823/clarity_CPC1_data/clarity_data/HA_outputs/train/S09844_L0231_E013_mono.wav'], return_hypotheses = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][0].y_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a[0][0].timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i thought we hadn't seen the last of her\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -96.6309,  -59.4936,  -50.2962,  ..., -101.5079,  -90.8408,\n",
      "           0.0000])\n"
     ]
    }
   ],
   "source": [
    "print(a[0][0].alignments[3][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sample_rate': 16000, 'compute_eval_loss': False, 'log_prediction': True, 'model_defaults': {'enc_hidden': 1024, 'pred_hidden': 640, 'joint_hidden': 640}, 'train_ds': {'manifest_filepath': [['/data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket1/tarred_audio_manifest.json'], ['/data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket2/tarred_audio_manifest.json'], ['/data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket3/tarred_audio_manifest.json'], ['/data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket4/tarred_audio_manifest.json'], ['/data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket5/tarred_audio_manifest.json'], ['/data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket6/tarred_audio_manifest.json'], ['/data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket7/tarred_audio_manifest.json'], ['/data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket8/tarred_audio_manifest.json']], 'sample_rate': 16000, 'batch_size': 1, 'shuffle': True, 'num_workers': 4, 'pin_memory': True, 'use_start_end_token': False, 'trim_silence': False, 'max_duration': 20, 'min_duration': 0, 'is_tarred': True, 'tarred_audio_filepaths': [['/data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket1/audio__OP_0..8191_CL_.tar'], ['/data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket2/audio__OP_0..8191_CL_.tar'], ['/data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket3/audio__OP_0..8191_CL_.tar'], ['/data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket4/audio__OP_0..8191_CL_.tar'], ['/data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket5/audio__OP_0..8191_CL_.tar'], ['/data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket6/audio__OP_0..8191_CL_.tar'], ['/data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket7/audio__OP_0..8191_CL_.tar'], ['/data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket8/audio__OP_0..8191_CL_.tar']], 'shuffle_n': 2048, 'bucketing_strategy': 'synced_randomized', 'bucketing_batch_size': [32, 32, 16, 16, 16, 16, 8, 8]}, 'validation_ds': {'manifest_filepath': ['/data/librispeech_withsp2/manifests/librivox-dev-other.json', '/data/librispeech_withsp2/manifests/librivox-dev-clean.json', '/data/librispeech_withsp2/manifests/librivox-test-other.json', '/data/librispeech_withsp2/manifests/librivox-test-clean.json'], 'sample_rate': 16000, 'batch_size': 4, 'shuffle': False, 'num_workers': 4, 'pin_memory': True, 'use_start_end_token': False, 'is_tarred': False, 'tarred_audio_filepaths': 'na', 'min_duration': 0}, 'test_ds': {'manifest_filepath': ['/data/librispeech_withsp2/manifests/librivox-test-other.json', '/data/librispeech_withsp2/manifests/librivox-dev-clean.json', '/data/librispeech_withsp2/manifests/librivox-dev-other.json', '/data/librispeech_withsp2/manifests/librivox-test-clean.json'], 'sample_rate': 16000, 'batch_size': 4, 'shuffle': False, 'num_workers': 4, 'pin_memory': True, 'use_start_end_token': False, 'is_tarred': False, 'tarred_audio_filepaths': 'na'}, 'tokenizer': {'dir': '/tokenizers/ASR/nemo_asrset/english/nemo_asrset_3.0/tokenizer_spe_unigram_v1024/', 'type': 'bpe', 'model_path': 'nemo:7e7171cb67b84bcbacd17ab0dfdcb494_tokenizer.model', 'vocab_path': 'nemo:4cbd669ef1c34c34a8cb2b55622f99b1_vocab.txt', 'spe_tokenizer_vocab': 'nemo:19dba6ef03c4462ea23cfa5ea1893004_tokenizer.vocab'}, 'preprocessor': {'_target_': 'nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor', 'sample_rate': 16000, 'normalize': 'per_feature', 'window_size': 0.025, 'window_stride': 0.01, 'window': 'hann', 'features': 80, 'n_fft': 512, 'frame_splicing': 1, 'dither': 1e-05, 'pad_to': 0}, 'spec_augment': {'_target_': 'nemo.collections.asr.modules.SpectrogramAugmentation', 'freq_masks': 2, 'time_masks': 10, 'freq_width': 27, 'time_width': 0.05}, 'encoder': {'_target_': 'nemo.collections.asr.modules.ConformerEncoder', 'feat_in': 80, 'feat_out': -1, 'n_layers': 24, 'd_model': 1024, 'subsampling': 'striding', 'subsampling_factor': 4, 'subsampling_conv_channels': 1024, 'ff_expansion_factor': 4, 'self_attention_model': 'rel_pos', 'n_heads': 8, 'att_context_size': [-1, -1], 'xscaling': True, 'untie_biases': True, 'pos_emb_max_len': 5000, 'conv_kernel_size': 5, 'conv_norm_type': 'batch_norm', 'dropout': 0.1, 'dropout_emb': 0.0, 'dropout_att': 0.1}, 'decoder': {'_target_': 'nemo.collections.asr.modules.RNNTDecoder', 'normalization_mode': None, 'random_state_sampling': False, 'blank_as_pad': True, 'prednet': {'pred_hidden': 640, 'pred_rnn_layers': 2, 't_max': None, 'dropout': 0.1}, 'vocab_size': 1024}, 'joint': {'_target_': 'nemo.collections.asr.modules.RNNTJoint', 'log_softmax': None, 'preserve_memory': False, 'fuse_loss_wer': True, 'fused_batch_size': 8, 'jointnet': {'joint_hidden': 640, 'activation': 'relu', 'dropout': 0.1, 'encoder_hidden': 1024, 'pred_hidden': 640}, 'num_classes': 1024, 'vocabulary': ['<unk>', 's', '▁the', '▁a', 't', '▁to', '▁and', '▁that', \"'\", '▁of', '▁i', '▁in', 'd', 'ed', 'ing', '▁it', '▁you', 'n', 'e', '▁is', 'y', 'er', '▁for', 'm', '▁was', '▁we', 're', 'ly', 'r', '▁he', '▁s', '▁be', 'c', '▁this', '▁so', '▁on', '▁have', 'g', 'a', '▁not', 'or', 'p', '▁they', '▁with', 'o', 'al', '▁but', '▁c', '▁are', '▁re', '▁there', 'ar', 'b', 'l', 'le', '▁know', '▁as', 'in', 'u', '▁what', '▁do', '▁or', '▁', '▁at', 'ation', 'w', 'll', 'f', '▁one', '▁e', '▁f', 'ri', '▁if', '▁b', 'ent', '▁all', '▁de', '▁can', 'it', '▁would', '▁me', 'il', '▁like', '▁an', 'ce', '▁no', '▁t', 'ment', 'i', 've', 'k', '▁from', '▁just', '▁by', '▁his', '▁had', 'ion', '▁think', 'es', 'ch', 'on', '▁about', 'th', '▁my', 'ck', 'en', '▁were', '▁your', 'ur', '▁will', 'ic', '▁go', '▁con', '▁she', 'li', 'an', '▁out', 'ter', '▁because', 'se', 'la', '▁when', '▁st', '▁has', '▁don', '▁which', 'ir', 'ra', '▁p', '▁two', '▁right', '▁court', '▁some', '▁pa', '▁pro', 'lo', 'ate', '▁time', '▁uh', 'te', '▁up', '▁our', '▁g', '▁case', '▁w', '▁her', '▁well', 'ro', '▁their', '▁who', '▁been', '▁yeah', '▁said', 'ers', '▁mo', '▁other', '▁see', 'ive', 'tion', 'el', '▁more', '▁se', 'vi', '▁any', '▁get', '▁also', '▁fa', '▁here', '▁un', '▁now', '▁co', '▁people', 'age', 'ist', 'ul', '▁how', '▁going', 'h', '▁very', '▁then', '▁could', 'de', '▁ma', 'ver', '▁mean', '▁su', 'id', 'ng', '▁lo', '▁sa', '▁la', '▁sp', '▁us', 'ol', '▁pre', 'ance', '▁want', '▁work', 'ld', '▁did', 'ge', '▁state', '▁po', '▁dis', 'ity', 'able', '▁say', '▁where', 'et', '▁ra', 'us', '▁li', 'ry', 'ction', '▁um', 'nd', 'ne', '▁even', '▁him', '▁ba', 'ad', '▁these', '▁part', '▁really', 'lu', '▁them', '▁those', '▁ho', 'v', '▁bo', '▁ex', 'ect', '▁over', '▁make', 'ru', '▁ro', 'is', '▁fi', '▁m', 'ut', 'ke', 'ence', '▁bu', '▁ch', '▁comp', 'me', 'ted', '▁en', '▁fact', 'z', '▁into', '▁need', 'ure', '▁way', 'ies', 'end', '▁mi', '▁take', 'un', '▁only', '▁should', 'as', '▁look', '▁three', 'one', '▁v', 'co', '▁com', '▁man', 'ta', '▁good', 'ated', '▁back', '▁first', '▁mr', '▁point', 'x', '▁new', 'ant', '▁vi', '▁dr', 'ff', '▁k', 'ci', '▁app', 'um', '▁n', '▁exp', '▁under', '▁after', 'po', 'ight', '▁d', 'est', 'mp', '▁okay', 'ine', 'ti', 'mo', 'qu', '▁da', '▁come', '▁day', '▁oh', '▁issue', 'at', '▁question', '▁than', '▁pri', 'ho', 'ten', '▁ca', '▁off', 'ain', '▁di', '▁o', '▁much', '▁five', 'ow', '▁ha', '▁thousand', '▁six', '▁give', '▁let', '▁le', 'ary', 'ous', 'sh', '▁yes', '▁four', '▁before', '▁call', '▁ga', '▁ne', '▁law', 'ma', '▁tra', 'ish', 'ical', '▁something', 'iv', '▁why', '▁may', '▁talk', '▁actually', 'ig', '▁act', '▁got', '▁car', '▁through', 'ful', '▁made', '▁hundred', 'om', 'man', 'ia', '▁didn', 'ual', '▁kind', '▁sta', 'ting', '▁things', 'ell', '▁down', 'im', '▁years', '▁lot', '▁ri', 'pe', 'ard', 'he', '▁gra', '▁year', '▁every', '▁eight', '▁reason', '▁seven', 'ca', 'ise', '▁twenty', '▁many', '▁per', 'op', '▁little', '▁mar', '▁same', '▁find', '▁different', '▁wa', 'pp', '▁nine', 'if', '▁does', 'ie', 'ach', 'cu', '▁never', '▁thing', '▁whether', 'ward', '▁again', '▁put', '▁claim', '▁op', 'ugh', '▁show', 'tic', 'ial', '▁still', '▁government', 'ok', '▁th', '▁long', '▁gu', '▁being', '▁plan', '▁own', 'am', 'ian', '▁evidence', 'ot', 'mb', '▁most', '▁ru', 'mit', '▁thank', 'ious', '▁person', 'em', '▁use', 'na', '▁sha', '▁r', '▁believe', 'mi', '▁school', '▁record', 'ition', '▁imp', 'ound', '▁place', 'ities', '▁happen', '▁ph', '▁great', 'ative', 'ha', 'no', '▁cha', 'ite', '▁clear', '▁help', 'dic', 'ea', '▁high', '▁judge', '▁al', '▁last', 'di', 'qui', '▁sure', 'tra', 'go', '▁sh', '▁both', '▁read', 'ving', '▁district', '▁ref', '▁sub', '▁hi', '▁such', '▁ten', '▁start', '▁doesn', 'ph', '▁change', 'ay', '▁count', '▁ob', '▁found', '▁du', '▁ki', '▁pay', '▁order', '▁another', 'for', '▁hand', 'lic', '▁number', '▁too', '▁ta', 'ni', 'ens', '▁tell', 'ven', 'port', '▁fun', '▁h', '▁term', '▁end', 'rr', '▁must', '▁home', '▁hu', '▁wi', 'side', '▁certain', '▁name', '▁understand', '▁money', 'ize', '▁pu', '▁interest', 'ip', '▁came', '▁house', 'land', '▁men', '▁require', '▁says', 'min', '▁member', 'our', 'ating', 'ber', '▁jo', '▁might', 'vo', '▁support', 'ries', 'son', '▁problem', 'ling', '▁old', '▁pe', '▁second', '▁def', 'ness', '▁decision', 'row', '▁dollars', '▁mu', '▁conf', 'use', '▁ah', '▁va', '▁life', '▁inter', '▁play', 'va', '▁saying', '▁pi', '▁far', '▁went', '▁public', '▁war', '▁trans', '▁live', '▁ask', '▁real', 'rate', '▁important', '▁thought', '▁done', '▁between', '▁argument', '▁rep', '▁consider', '▁anything', '▁direct', 'less', '▁ja', '▁doing', '▁course', 'line', '▁month', '▁word', '▁care', '▁provide', 'ap', '▁honor', 'ba', 'ator', 'j', '▁report', '▁exc', '▁service', '▁tri', 'nk', '▁world', '▁without', '▁ju', '▁against', 'ible', '▁used', '▁always', '▁able', 'ship', '▁rule', '▁matter', '▁while', 'par', '▁maybe', '▁miss', '▁around', '▁u', '▁today', '▁friend', '▁ti', '▁big', 'bu', '▁next', 'ju', '▁thirty', '▁pass', '▁y', '▁business', '▁test', '▁fo', '▁nothing', '▁turn', '▁process', 'com', '▁el', '▁better', '▁commission', '▁week', '▁move', '▁trial', '▁open', '▁each', '▁specific', '▁feel', '▁cap', 'ency', '▁particular', 'ability', '▁system', 'gra', '▁present', '▁looking', 'duc', '▁fra', '▁correct', '▁allow', '▁side', 'ign', 'serv', '▁city', '▁agree', '▁having', '▁defendant', '▁keep', '▁follow', '▁invest', '▁add', '▁information', '▁whole', '▁general', 'hi', '▁away', '▁upon', '▁win', '▁based', '▁program', '▁develop', '▁country', 'ified', '▁sort', '▁board', '▁qua', 'ex', '▁family', '▁close', '▁dec', '▁though', '▁hard', '▁answer', '▁guess', '▁job', '▁trying', '▁je', '▁asked', '▁during', '▁fee', '▁love', '▁already', '▁address', '▁ever', '▁bill', 'gue', '▁cause', '▁gen', '▁free', '▁continue', '▁fifty', '▁contract', '▁european', '▁rest', '▁few', '▁water', '▁line', '▁position', '▁percent', '▁enough', '▁sea', '▁har', '▁america', '▁full', '▁since', '▁everything', '▁bring', '▁lead', '▁probably', '▁children', '▁tax', '▁face', '▁best', '▁concern', '▁sign', '▁mhm', '▁minute', '▁bit', '▁stand', '▁situation', '▁speak', '▁hope', '▁yet', '▁discuss', '▁standard', '▁brief', '▁statute', '▁officer', '▁ok', '▁respect', '▁together', '▁agreement', '▁room', '▁least', '▁market', '▁however', '▁getting', '▁else', '▁true', '▁counsel', 'ttle', '▁err', '▁effect', '▁example', '▁less', '▁cost', '▁left', '▁group', '▁level', '▁policy', '▁town', '▁small', '▁child', '▁president', '▁large', '▁later', '▁protect', '▁decide', 'way', '▁making', '▁regard', '▁book', '▁pretty', '▁author', '▁hour', '▁idea', '▁include', '▁exactly', '▁power', '▁limit', '▁watch', '▁night', '▁ya', '▁jury', '▁leave', '▁quite', '▁cover', '▁grow', '▁result', '▁singapore', '▁either', '▁company', '▁stop', '▁community', '▁million', '▁current', '▁federal', '▁united', '▁mind', '▁step', '▁class', '▁police', '▁remain', '▁health', '▁final', '▁represent', '▁light', '▁treat', '▁review', '▁testimony', '▁bank', '▁product', '▁appeal', '▁half', '▁cannot', '▁involve', '▁charge', '▁knew', '▁learn', '▁eye', '▁language', '▁individual', '▁receive', '▁further', '▁mister', '▁view', '▁council', '▁until', '▁human', '▁stuff', '▁student', '▁become', '▁walk', '▁control', '▁check', '▁seems', '▁strong', '▁period', '▁please', '▁food', '▁type', '▁complete', '▁purpose', '▁account', '▁project', '▁sense', '▁national', '▁appear', '▁simply', '▁return', '▁benefit', '▁employ', '▁create', '▁wrong', '▁whatever', '▁morning', '▁suggest', '▁condition', '▁amount', '▁object', '▁third', '▁remember', '▁himself', '▁property', '▁itself', '▁somebody', '▁suppose', '▁future', '▁short', '▁opinion', '▁several', '▁common', '▁taking', '▁notice', '▁office', '▁almost', '▁father', '▁document', '▁judgment', '▁experience', '▁uhhuh', '▁committee', '▁parent', '▁legal', '▁client', '▁major', '▁congress', '▁rather', '▁proceed', '▁possible', '▁plea', '▁subject', '▁along', '▁mother', '▁increase', '▁meeting', '▁girl', '▁couple', '▁north', '▁local', '▁sometimes', '▁brought', '▁fifteen', '▁organ', '▁difference', '▁entire', '▁security', '▁department', '▁south', '▁vote', '▁basis', '▁twelve', '▁conduct', '▁street', '▁social', '▁wonder', '▁supreme', '▁difficult', '▁travel', '▁opportunity', '▁defense', '▁plaintiff', '▁white', '▁focus', '▁therefore', '▁challenge', '▁value', '▁except', '▁among', '▁ground', '▁serve', '▁attorney', '▁education', '▁credit', '▁request', '▁women', '▁basically', '▁political', '▁determine', '▁access', '▁across', '▁sentence', '▁perform', '▁everybody', '▁black', '▁drug', '▁accept', '▁eleven', '▁establish', '▁design', '▁success', '▁including', '▁john', '▁employee', '▁nice', '▁crime', 'ibility', '▁circumstance', '▁occur', '▁citizen', '▁woman', '▁witness', '▁explain', '▁additional', '▁history', '▁front', '▁medical', '▁death', 'ification', '▁financial', '▁break', '▁connect', '▁perhaps', '▁sorry', '▁especially', '▁trust', '▁insurance', '▁economic', '▁effort', '▁circuit', '▁cross', '▁instead', '▁affect', '▁zero', '▁available', '▁team', '▁application', '▁separate', '▁absolutely', '▁christ', 'ization', '▁europe', '▁budget', '▁damage', '▁myself', '▁private', '▁figure', '▁provision', '▁collect', '▁similar', '▁regulation', '▁approach', '▁themselves', '▁construct', '▁petition', '▁material', '▁improve', '▁york', '▁parliament', '▁broad', '▁companies', '▁nature', '▁piece', '▁threat', '▁energy', '▁behind', '▁significant', '▁response', '▁definitely', '▁guide', '▁beyond', '▁amendment', '▁measure', '▁criminal', '▁original', '▁brother', '▁impact', 'q']}, 'decoding': {'strategy': 'greedy_batch', 'greedy': {'max_symbols': 5}, 'beam': {'beam_size': 2, 'return_best_hypothesis': False, 'score_norm': True, 'tsd_max_sym_exp': 50, 'alsd_max_target_len': 2.0}}, 'loss': {'loss_name': 'default', 'warprnnt_numba_kwargs': {'fastemit_lambda': 0.0, 'clamp': -1.0}}, 'variational_noise': {'start_step': 0, 'std': 0}, 'optim': {'name': 'adamw', 'lr': 6.4, 'betas': [0.9, 0.98], 'weight_decay': 0.001, 'sched': {'name': 'NoamAnnealing', 'd_model': 1024, 'warmup_steps': 10000, 'warmup_ratio': None, 'min_lr': 1e-06, 'max_steps': 200000}}, 'labels': ['<unk>', 's', '▁the', '▁a', 't', '▁to', '▁and', '▁that', \"'\", '▁of', '▁i', '▁in', 'd', 'ed', 'ing', '▁it', '▁you', 'n', 'e', '▁is', 'y', 'er', '▁for', 'm', '▁was', '▁we', 're', 'ly', 'r', '▁he', '▁s', '▁be', 'c', '▁this', '▁so', '▁on', '▁have', 'g', 'a', '▁not', 'or', 'p', '▁they', '▁with', 'o', 'al', '▁but', '▁c', '▁are', '▁re', '▁there', 'ar', 'b', 'l', 'le', '▁know', '▁as', 'in', 'u', '▁what', '▁do', '▁or', '▁', '▁at', 'ation', 'w', 'll', 'f', '▁one', '▁e', '▁f', 'ri', '▁if', '▁b', 'ent', '▁all', '▁de', '▁can', 'it', '▁would', '▁me', 'il', '▁like', '▁an', 'ce', '▁no', '▁t', 'ment', 'i', 've', 'k', '▁from', '▁just', '▁by', '▁his', '▁had', 'ion', '▁think', 'es', 'ch', 'on', '▁about', 'th', '▁my', 'ck', 'en', '▁were', '▁your', 'ur', '▁will', 'ic', '▁go', '▁con', '▁she', 'li', 'an', '▁out', 'ter', '▁because', 'se', 'la', '▁when', '▁st', '▁has', '▁don', '▁which', 'ir', 'ra', '▁p', '▁two', '▁right', '▁court', '▁some', '▁pa', '▁pro', 'lo', 'ate', '▁time', '▁uh', 'te', '▁up', '▁our', '▁g', '▁case', '▁w', '▁her', '▁well', 'ro', '▁their', '▁who', '▁been', '▁yeah', '▁said', 'ers', '▁mo', '▁other', '▁see', 'ive', 'tion', 'el', '▁more', '▁se', 'vi', '▁any', '▁get', '▁also', '▁fa', '▁here', '▁un', '▁now', '▁co', '▁people', 'age', 'ist', 'ul', '▁how', '▁going', 'h', '▁very', '▁then', '▁could', 'de', '▁ma', 'ver', '▁mean', '▁su', 'id', 'ng', '▁lo', '▁sa', '▁la', '▁sp', '▁us', 'ol', '▁pre', 'ance', '▁want', '▁work', 'ld', '▁did', 'ge', '▁state', '▁po', '▁dis', 'ity', 'able', '▁say', '▁where', 'et', '▁ra', 'us', '▁li', 'ry', 'ction', '▁um', 'nd', 'ne', '▁even', '▁him', '▁ba', 'ad', '▁these', '▁part', '▁really', 'lu', '▁them', '▁those', '▁ho', 'v', '▁bo', '▁ex', 'ect', '▁over', '▁make', 'ru', '▁ro', 'is', '▁fi', '▁m', 'ut', 'ke', 'ence', '▁bu', '▁ch', '▁comp', 'me', 'ted', '▁en', '▁fact', 'z', '▁into', '▁need', 'ure', '▁way', 'ies', 'end', '▁mi', '▁take', 'un', '▁only', '▁should', 'as', '▁look', '▁three', 'one', '▁v', 'co', '▁com', '▁man', 'ta', '▁good', 'ated', '▁back', '▁first', '▁mr', '▁point', 'x', '▁new', 'ant', '▁vi', '▁dr', 'ff', '▁k', 'ci', '▁app', 'um', '▁n', '▁exp', '▁under', '▁after', 'po', 'ight', '▁d', 'est', 'mp', '▁okay', 'ine', 'ti', 'mo', 'qu', '▁da', '▁come', '▁day', '▁oh', '▁issue', 'at', '▁question', '▁than', '▁pri', 'ho', 'ten', '▁ca', '▁off', 'ain', '▁di', '▁o', '▁much', '▁five', 'ow', '▁ha', '▁thousand', '▁six', '▁give', '▁let', '▁le', 'ary', 'ous', 'sh', '▁yes', '▁four', '▁before', '▁call', '▁ga', '▁ne', '▁law', 'ma', '▁tra', 'ish', 'ical', '▁something', 'iv', '▁why', '▁may', '▁talk', '▁actually', 'ig', '▁act', '▁got', '▁car', '▁through', 'ful', '▁made', '▁hundred', 'om', 'man', 'ia', '▁didn', 'ual', '▁kind', '▁sta', 'ting', '▁things', 'ell', '▁down', 'im', '▁years', '▁lot', '▁ri', 'pe', 'ard', 'he', '▁gra', '▁year', '▁every', '▁eight', '▁reason', '▁seven', 'ca', 'ise', '▁twenty', '▁many', '▁per', 'op', '▁little', '▁mar', '▁same', '▁find', '▁different', '▁wa', 'pp', '▁nine', 'if', '▁does', 'ie', 'ach', 'cu', '▁never', '▁thing', '▁whether', 'ward', '▁again', '▁put', '▁claim', '▁op', 'ugh', '▁show', 'tic', 'ial', '▁still', '▁government', 'ok', '▁th', '▁long', '▁gu', '▁being', '▁plan', '▁own', 'am', 'ian', '▁evidence', 'ot', 'mb', '▁most', '▁ru', 'mit', '▁thank', 'ious', '▁person', 'em', '▁use', 'na', '▁sha', '▁r', '▁believe', 'mi', '▁school', '▁record', 'ition', '▁imp', 'ound', '▁place', 'ities', '▁happen', '▁ph', '▁great', 'ative', 'ha', 'no', '▁cha', 'ite', '▁clear', '▁help', 'dic', 'ea', '▁high', '▁judge', '▁al', '▁last', 'di', 'qui', '▁sure', 'tra', 'go', '▁sh', '▁both', '▁read', 'ving', '▁district', '▁ref', '▁sub', '▁hi', '▁such', '▁ten', '▁start', '▁doesn', 'ph', '▁change', 'ay', '▁count', '▁ob', '▁found', '▁du', '▁ki', '▁pay', '▁order', '▁another', 'for', '▁hand', 'lic', '▁number', '▁too', '▁ta', 'ni', 'ens', '▁tell', 'ven', 'port', '▁fun', '▁h', '▁term', '▁end', 'rr', '▁must', '▁home', '▁hu', '▁wi', 'side', '▁certain', '▁name', '▁understand', '▁money', 'ize', '▁pu', '▁interest', 'ip', '▁came', '▁house', 'land', '▁men', '▁require', '▁says', 'min', '▁member', 'our', 'ating', 'ber', '▁jo', '▁might', 'vo', '▁support', 'ries', 'son', '▁problem', 'ling', '▁old', '▁pe', '▁second', '▁def', 'ness', '▁decision', 'row', '▁dollars', '▁mu', '▁conf', 'use', '▁ah', '▁va', '▁life', '▁inter', '▁play', 'va', '▁saying', '▁pi', '▁far', '▁went', '▁public', '▁war', '▁trans', '▁live', '▁ask', '▁real', 'rate', '▁important', '▁thought', '▁done', '▁between', '▁argument', '▁rep', '▁consider', '▁anything', '▁direct', 'less', '▁ja', '▁doing', '▁course', 'line', '▁month', '▁word', '▁care', '▁provide', 'ap', '▁honor', 'ba', 'ator', 'j', '▁report', '▁exc', '▁service', '▁tri', 'nk', '▁world', '▁without', '▁ju', '▁against', 'ible', '▁used', '▁always', '▁able', 'ship', '▁rule', '▁matter', '▁while', 'par', '▁maybe', '▁miss', '▁around', '▁u', '▁today', '▁friend', '▁ti', '▁big', 'bu', '▁next', 'ju', '▁thirty', '▁pass', '▁y', '▁business', '▁test', '▁fo', '▁nothing', '▁turn', '▁process', 'com', '▁el', '▁better', '▁commission', '▁week', '▁move', '▁trial', '▁open', '▁each', '▁specific', '▁feel', '▁cap', 'ency', '▁particular', 'ability', '▁system', 'gra', '▁present', '▁looking', 'duc', '▁fra', '▁correct', '▁allow', '▁side', 'ign', 'serv', '▁city', '▁agree', '▁having', '▁defendant', '▁keep', '▁follow', '▁invest', '▁add', '▁information', '▁whole', '▁general', 'hi', '▁away', '▁upon', '▁win', '▁based', '▁program', '▁develop', '▁country', 'ified', '▁sort', '▁board', '▁qua', 'ex', '▁family', '▁close', '▁dec', '▁though', '▁hard', '▁answer', '▁guess', '▁job', '▁trying', '▁je', '▁asked', '▁during', '▁fee', '▁love', '▁already', '▁address', '▁ever', '▁bill', 'gue', '▁cause', '▁gen', '▁free', '▁continue', '▁fifty', '▁contract', '▁european', '▁rest', '▁few', '▁water', '▁line', '▁position', '▁percent', '▁enough', '▁sea', '▁har', '▁america', '▁full', '▁since', '▁everything', '▁bring', '▁lead', '▁probably', '▁children', '▁tax', '▁face', '▁best', '▁concern', '▁sign', '▁mhm', '▁minute', '▁bit', '▁stand', '▁situation', '▁speak', '▁hope', '▁yet', '▁discuss', '▁standard', '▁brief', '▁statute', '▁officer', '▁ok', '▁respect', '▁together', '▁agreement', '▁room', '▁least', '▁market', '▁however', '▁getting', '▁else', '▁true', '▁counsel', 'ttle', '▁err', '▁effect', '▁example', '▁less', '▁cost', '▁left', '▁group', '▁level', '▁policy', '▁town', '▁small', '▁child', '▁president', '▁large', '▁later', '▁protect', '▁decide', 'way', '▁making', '▁regard', '▁book', '▁pretty', '▁author', '▁hour', '▁idea', '▁include', '▁exactly', '▁power', '▁limit', '▁watch', '▁night', '▁ya', '▁jury', '▁leave', '▁quite', '▁cover', '▁grow', '▁result', '▁singapore', '▁either', '▁company', '▁stop', '▁community', '▁million', '▁current', '▁federal', '▁united', '▁mind', '▁step', '▁class', '▁police', '▁remain', '▁health', '▁final', '▁represent', '▁light', '▁treat', '▁review', '▁testimony', '▁bank', '▁product', '▁appeal', '▁half', '▁cannot', '▁involve', '▁charge', '▁knew', '▁learn', '▁eye', '▁language', '▁individual', '▁receive', '▁further', '▁mister', '▁view', '▁council', '▁until', '▁human', '▁stuff', '▁student', '▁become', '▁walk', '▁control', '▁check', '▁seems', '▁strong', '▁period', '▁please', '▁food', '▁type', '▁complete', '▁purpose', '▁account', '▁project', '▁sense', '▁national', '▁appear', '▁simply', '▁return', '▁benefit', '▁employ', '▁create', '▁wrong', '▁whatever', '▁morning', '▁suggest', '▁condition', '▁amount', '▁object', '▁third', '▁remember', '▁himself', '▁property', '▁itself', '▁somebody', '▁suppose', '▁future', '▁short', '▁opinion', '▁several', '▁common', '▁taking', '▁notice', '▁office', '▁almost', '▁father', '▁document', '▁judgment', '▁experience', '▁uhhuh', '▁committee', '▁parent', '▁legal', '▁client', '▁major', '▁congress', '▁rather', '▁proceed', '▁possible', '▁plea', '▁subject', '▁along', '▁mother', '▁increase', '▁meeting', '▁girl', '▁couple', '▁north', '▁local', '▁sometimes', '▁brought', '▁fifteen', '▁organ', '▁difference', '▁entire', '▁security', '▁department', '▁south', '▁vote', '▁basis', '▁twelve', '▁conduct', '▁street', '▁social', '▁wonder', '▁supreme', '▁difficult', '▁travel', '▁opportunity', '▁defense', '▁plaintiff', '▁white', '▁focus', '▁therefore', '▁challenge', '▁value', '▁except', '▁among', '▁ground', '▁serve', '▁attorney', '▁education', '▁credit', '▁request', '▁women', '▁basically', '▁political', '▁determine', '▁access', '▁across', '▁sentence', '▁perform', '▁everybody', '▁black', '▁drug', '▁accept', '▁eleven', '▁establish', '▁design', '▁success', '▁including', '▁john', '▁employee', '▁nice', '▁crime', 'ibility', '▁circumstance', '▁occur', '▁citizen', '▁woman', '▁witness', '▁explain', '▁additional', '▁history', '▁front', '▁medical', '▁death', 'ification', '▁financial', '▁break', '▁connect', '▁perhaps', '▁sorry', '▁especially', '▁trust', '▁insurance', '▁economic', '▁effort', '▁circuit', '▁cross', '▁instead', '▁affect', '▁zero', '▁available', '▁team', '▁application', '▁separate', '▁absolutely', '▁christ', 'ization', '▁europe', '▁budget', '▁damage', '▁myself', '▁private', '▁figure', '▁provision', '▁collect', '▁similar', '▁regulation', '▁approach', '▁themselves', '▁construct', '▁petition', '▁material', '▁improve', '▁york', '▁parliament', '▁broad', '▁companies', '▁nature', '▁piece', '▁threat', '▁energy', '▁behind', '▁significant', '▁response', '▁definitely', '▁guide', '▁beyond', '▁amendment', '▁measure', '▁criminal', '▁original', '▁brother', '▁impact', 'q'], 'target': 'nemo.collections.asr.models.rnnt_bpe_models.EncDecRNNTBPEModel', 'nemo_version': '1.8.0rc0'}\n"
     ]
    }
   ],
   "source": [
    "print(model.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
