{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import openpyxl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.init as init\n",
    "from transformers import Wav2Vec2Model\n",
    "import sys\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "# model = nemo_asr.models.EncDecRNNTModel.from_pretrained(\"nvidia/stt_en_conformer_transducer_xlarge\")\n",
    "model = nemo_asr.models.EncDecRNNTModel.from_pretrained(\"nvidia/stt_en_conformer_transducer_xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = model.transcribe(paths2audio_files = [\"/home/ubuntu/elec823/clean.wav\", \"/home/ubuntu/elec823/clean2.wav\"], return_hypotheses = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = a.word_confidence\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from utils import *\n",
    "from my_loss import *\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "# model = nemo_asr.models.EncDecRNNTModel.from_pretrained(\"nvidia/stt_en_conformer_transducer_xlarge\")\n",
    "model = nemo_asr.models.EncDecRNNTModel.from_pretrained(\"nvidia/stt_en_conformer_transducer_xlarge\")\n",
    "\n",
    "MODEL = '01_mel_mono_freeze'\n",
    "\n",
    "CONSTANTS = InitializationTrain(\n",
    "    model_name=MODEL, \n",
    "    verbose=True,\n",
    "    train_data_path='/home/ubuntu/elec823/clarity_CPC1_data/clarity_data/HA_outputs/train/',\n",
    "    train_info_path='/home/ubuntu/elec823/clarity_CPC1_data/metadata/CPC1.train.json',\n",
    "    train_audiogram_path='/home/ubuntu/elec823/clarity_CPC1_data/metadata/listeners.CPC1_train.json',\n",
    "    train_listener_path='/home/ubuntu/elec823/clarity_CPC1_data/metadata/listener_data.CPC1_train.xlsx',\n",
    "    log_path='/home/ubuntu/elec823/log/',\n",
    "    save_path='/home/ubuntu/elec823/checkpoints/',\n",
    "    orig_freq=32000,\n",
    "    new_freq=16_000,\n",
    "    seed=3407,\n",
    "    device=None\n",
    "    )\n",
    "dataset = CPCdata(metadata=CONSTANTS.metadata)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    train_loader = DataLoader(dataset=[dataset[i] for i in train_idx], batch_size=3, shuffle=True, pin_memory=True,num_workers=8)\n",
    "    break\n",
    "for speech_input, info_dict in train_loader:\n",
    "    mono_path = [CONSTANTS.DATA_PATH + path + \"_mono.wav\" for path in info_dict['path']]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_path = ['/home/ubuntu/elec823/clarity_CPC1_data/clarity_data/HA_outputs/train/S09844_L0231_E013_mono.wav',\n",
    " '/home/ubuntu/elec823/clarity_CPC1_data/clarity_data/HA_outputs/train/S09928_L0219_E021_mono.wav',\n",
    " '/home/ubuntu/elec823/clarity_CPC1_data/clarity_data/HA_outputs/train/S09463_L0209_E013_mono.wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.transcribe(paths2audio_files = converted_path, return_hypotheses = True)\n",
    "# Use out[0]\n",
    "# then will be a list of 'batch' hypotheses\n",
    "# word confidence is out[0][i].word_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = [out[0][i].word_confidence for i in range(len(out[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(confidence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = confidence\n",
    "def truncate_and_pad(tensor):\n",
    "    FIXED_LENGTH = 10\n",
    "    if type(tensor) is not torch.Tensor:\n",
    "        tensor = torch.tensor(tensor)\n",
    "    current_length = tensor.size(0)\n",
    "\n",
    "    # If the tensor length is greater than the target length, truncate it\n",
    "    if current_length > FIXED_LENGTH:\n",
    "        tensor = tensor[:FIXED_LENGTH]\n",
    "    # If the tensor length is less than the target length, pad it with zeros\n",
    "    elif current_length < FIXED_LENGTH:\n",
    "        pad_size = FIXED_LENGTH - current_length\n",
    "        padding = torch.zeros(pad_size, dtype=tensor.dtype, device=tensor.device)\n",
    "        tensor = torch.cat((tensor, padding), dim=0)\n",
    "    return tensor\n",
    "\n",
    "result = list(map(truncate_and_pad, t1))\n",
    "print(result)\n",
    "print(len(result))\n",
    "print(len(result[0]))\n",
    "print(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.stack(result, dim=0)\n",
    "print(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.LSTM(input_size=10,\n",
    "                    hidden_size=256,\n",
    "                    num_layers=3,\n",
    "                    batch_first=True,\n",
    "                    bidirectional=True)\n",
    "out, (h_n, c_n) = m(a)\n",
    "print(out.shape)\n",
    "m2 = nn.Linear(512, 1)\n",
    "m2(out)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(\"nvidia/stt_en_conformer_transducer_xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence, _ = asr_model.transcribe(paths2audio_files = converted_path, return_hypotheses = True)\n",
    "confidence = [confidence[i].word_confidence for i in range(len(confidence))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confidence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = torch.stack(list(map(truncate_and_pad, confidence)), dim=0)\n",
    "print(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_lstm_predictor = nn.Sequential(\n",
    "            nn.LSTM(input_size=10,\n",
    "                    hidden_size=256,\n",
    "                    num_layers=3,\n",
    "                    batch_first=True,\n",
    "                    bidirectional=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "t1 = torch.randn(3, 10)\n",
    "t2 = bidirectional_lstm_predictor(t1)\n",
    "print(t2)\n",
    "out = bidirectional_lstm_predictor(confidence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-03-27 03:48:50 mixins:170] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-03-27 03:48:52 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath:\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket1/tarred_audio_manifest.json\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket2/tarred_audio_manifest.json\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket3/tarred_audio_manifest.json\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket4/tarred_audio_manifest.json\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket5/tarred_audio_manifest.json\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket6/tarred_audio_manifest.json\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket7/tarred_audio_manifest.json\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket8/tarred_audio_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 1\n",
      "    shuffle: true\n",
      "    num_workers: 4\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20\n",
      "    min_duration: 0\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths:\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket1/audio__OP_0..8191_CL_.tar\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket2/audio__OP_0..8191_CL_.tar\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket3/audio__OP_0..8191_CL_.tar\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket4/audio__OP_0..8191_CL_.tar\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket5/audio__OP_0..8191_CL_.tar\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket6/audio__OP_0..8191_CL_.tar\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket7/audio__OP_0..8191_CL_.tar\n",
      "    - - /data/NeMo_ASR_SET/English/v3.0/train_bucketed/bucket8/audio__OP_0..8191_CL_.tar\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: synced_randomized\n",
      "    bucketing_batch_size:\n",
      "    - 32\n",
      "    - 32\n",
      "    - 16\n",
      "    - 16\n",
      "    - 16\n",
      "    - 16\n",
      "    - 8\n",
      "    - 8\n",
      "    \n",
      "[NeMo W 2023-03-27 03:48:52 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - /data/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 4\n",
      "    shuffle: false\n",
      "    num_workers: 4\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    min_duration: 0\n",
      "    \n",
      "[NeMo W 2023-03-27 03:48:52 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /data/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 4\n",
      "    shuffle: false\n",
      "    num_workers: 4\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-03-27 03:48:52 features:286] PADDING: 0\n",
      "[NeMo I 2023-03-27 03:48:56 rnnt_models:206] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2023-03-27 03:49:06 save_restore_connector:247] Model EncDecRNNTBPEModel was successfully restored from /home/ubuntu/.cache/huggingface/hub/models--nvidia--stt_en_conformer_transducer_xlarge/snapshots/96472b7552a5d0559a22399ea300498c5412699f/stt_en_conformer_transducer_xlarge.nemo.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import openpyxl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.init as init\n",
    "from transformers import Wav2Vec2Model\n",
    "import sys\n",
    "\n",
    "from utils import *\n",
    "\n",
    "class WordConfidence(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WordConfidence, self).__init__()\n",
    "        self.asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(\"nvidia/stt_en_conformer_transducer_xlarge\")\n",
    "        self.bidirectional_lstm=nn.LSTM(\n",
    "            input_size=10,\n",
    "            hidden_size=256,\n",
    "            num_layers=3,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "            )\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, speech_input, meta_data, mono_path):\n",
    "        # output of asr model is [B, word_len]\n",
    "        confidence, _ = self.asr_model.transcribe(paths2audio_files = mono_path, return_hypotheses = True)\n",
    "        confidence = [confidence[i].word_confidence for i in range(len(confidence))]\n",
    "        # padding and truncating to 10: output shape [B, 10]\n",
    "        confidence = torch.stack(list(map(self.truncate_and_pad, confidence)), dim=0)\n",
    "        lstm_out, (h_n, c_n) = self.bidirectional_lstm(confidence)\n",
    "        pred = self.predictor(lstm_out)\n",
    "        \n",
    "        return pred\n",
    "\n",
    "        \n",
    "    def truncate_and_pad(self, tensor):\n",
    "        FIXED_LENGTH = 10\n",
    "        if type(tensor) is not torch.Tensor:\n",
    "            tensor = torch.tensor(tensor)\n",
    "        current_length = tensor.size(0)\n",
    "\n",
    "        # If the tensor length is greater than the target length, truncate it\n",
    "        if current_length > FIXED_LENGTH:\n",
    "            tensor = tensor[:FIXED_LENGTH]\n",
    "        # If the tensor length is less than the target length, pad it with zeros\n",
    "        elif current_length < FIXED_LENGTH:\n",
    "            pad_size = FIXED_LENGTH - current_length\n",
    "            padding = torch.zeros(pad_size, dtype=tensor.dtype, device=tensor.device)\n",
    "            tensor = torch.cat((tensor, padding), dim=0)\n",
    "        return tensor\n",
    "model = WordConfidence()\n",
    "converted_path = ['/home/ubuntu/elec823/clarity_CPC1_data/clarity_data/HA_outputs/train/S09844_L0231_E013_mono.wav',\n",
    " '/home/ubuntu/elec823/clarity_CPC1_data/clarity_data/HA_outputs/train/S09928_L0219_E021_mono.wav',\n",
    " '/home/ubuntu/elec823/clarity_CPC1_data/clarity_data/HA_outputs/train/S09463_L0209_E013_mono.wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14117f06fa3644a193979a48b933e88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = model(1,1,converted_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4975],\n",
      "        [0.4984],\n",
      "        [0.4993]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
