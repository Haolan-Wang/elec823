{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import os\n",
    "from utils import *\n",
    "from models import *\n",
    "from my_loss import *\n",
    "from data_process import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(\n",
    "            \"nvidia/stt_en_conformer_transducer_xlarge\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j  in asr_model.cfg.items():\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSTANTS = InitializationTrain(\n",
    "    verbose=True\n",
    ")\n",
    "dataset = CPCdataBinaural(metadata=CONSTANTS.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderPredictor().to(CONSTANTS.device)\n",
    "mel = model.logmel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset, batch_size=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listener Info (Audiogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listener_info = ListenerInfo(['L0231', 'L0201'])\n",
    "audiogram_l = [listener_info.info[i]['audiogram_l'] for i in range(len(listener_info.info))]\n",
    "audiogram_r = [listener_info.info[i]['audiogram_r'] for i in range(len(listener_info.info))]\n",
    "audiogram_cfs = [listener_info.info[i]['audiogram_cfs'] for i in range(len(listener_info.info))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiogram_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listener_info = ListenerInfo(['L0231', 'L0201'])\n",
    "print(listener_info.info)\n",
    "print(listener_info.info[0]['audiogram_l'])\n",
    "print(listener_info.info[0]['audiogram_r'])\n",
    "print(listener_info.info[0]['audiogram_cfs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for speech_input_l, speech_input_r, info_dict in tqdm(train_loader, desc=\"Training:\"):\n",
    "    mel_feature_l, mel_feature_length = mel(\n",
    "            input_signal=speech_input_l.to(device),\n",
    "            length=torch.full((speech_input_l.shape[0],), speech_input_l.shape[1]).to(device),\n",
    "        )\n",
    "    listener_info = ListenerInfo(info_dict['listener'])\n",
    "    listener_info.info['audiogram_l']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.asr_model.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item, value in model.asr_model.cfg.items():\n",
    "    print(item, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item, value in model.asr_model.cfg['preprocessor'].items():\n",
    "    print(item, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# 示例数据\n",
    "a = np.array([0, 1, 2, 3, 4, 5, 6, 7])  # 8元素的频率数组\n",
    "b = np.array([10, 20, 30, 40, 50, 60, 70, 80])  # 8元素的值数组\n",
    "c = np.linspace(0, 7, 80)  # 80元素的频率数组\n",
    "\n",
    "# 创建线性插值函数\n",
    "linear_interpolation = interp1d(a, b)\n",
    "\n",
    "# 计算c中每个频率对应的值\n",
    "result = linear_interpolation(c)\n",
    "\n",
    "# 打印结果\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mel_to_hz(mel):\n",
    "    return 700 * (10**(mel / 2595) - 1)\n",
    "\n",
    "def hz_to_mel(hz):\n",
    "    return 2595 * np.log10(1 + hz / 700)\n",
    "\n",
    "def get_central_frequencies(nfilt, lowfreq, highfreq):\n",
    "    low_mel = hz_to_mel(lowfreq)\n",
    "    high_mel = hz_to_mel(highfreq)\n",
    "\n",
    "    mel_points = np.linspace(low_mel, high_mel, nfilt + 2)  # nfilt + 2 points to include bounds\n",
    "    hz_points = mel_to_hz(mel_points)\n",
    "\n",
    "    central_frequencies = hz_points[1:-1]  # exclude the first and last points\n",
    "    return central_frequencies\n",
    "\n",
    "\n",
    "nfilt = 80\n",
    "lowfreq = 0\n",
    "highfreq = 8000\n",
    "\n",
    "central_frequencies = get_central_frequencies(nfilt, lowfreq, highfreq)\n",
    "print(central_frequencies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(central_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HurricaneData(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        for label_folder in os.listdir(root_dir):\n",
    "            label_path = os.path.join(root_dir, label_folder)\n",
    "            if os.path.isdir(label_path):\n",
    "                for audio_file in os.listdir(os.path.join(label_path,'ssn')):\n",
    "                    audio_path = os.path.join(label_path, audio_file)\n",
    "                    self.samples.append(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_dir = '/home/ubuntu/elec823/hurricane'\n",
    "print(os.listdir(root_dir))\n",
    "label_folder = os.listdir(root_dir)[0]\n",
    "label_path = os.path.join(root_dir, label_folder)\n",
    "print(label_path)\n",
    "print(os.path.join(label_path,'ssn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mod_folder in os.listdir(root_dir):\n",
    "    ssn_path = os.path.join(root_dir, mod_folder, 'ssn')\n",
    "print(os.listdir(ssn_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "root_dir = '/home/ubuntu/elec823/hurricane'\n",
    "samples = []\n",
    "for mod_folder in os.listdir(root_dir):\n",
    "    if mod_folder.startswith(\".\"):\n",
    "        continue\n",
    "    ssn_path = os.path.join(root_dir, mod_folder, 'ssn')\n",
    "    # print(ssn_path)\n",
    "    for snr in os.listdir(ssn_path):\n",
    "        if snr.startswith(\".\"):\n",
    "            continue\n",
    "        snr_path = os.path.join(ssn_path, snr)\n",
    "        for audio_file in os.listdir(snr_path):\n",
    "            audio_path = os.path.join(snr_path, audio_file)\n",
    "            samples.append(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(samples))\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "waveform, sample_rate = torchaudio.load(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform.shape\n",
    "import torch\n",
    "a = torch.mean(waveform, dim=0)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(label_path,'ssn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HurricaneData(Dataset):\n",
    "    def __init__(self, state, root_dir='/home/ubuntu/elec823/hurricane', transform=None):\n",
    "        self.state = state\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.scores = scipy.io.loadmat(os.path.join(root_dir, 'scores.mat'))['intell']\n",
    "        self.all_samples = []\n",
    "        self.noise_types = {\"cs\":0, \"ssn\":1}\n",
    "        self.snrs = {\"snrHi\":0, \"snrMid\":1, \"snrLo\":2}\n",
    "\n",
    "        for mod_folder in os.listdir(root_dir):\n",
    "            if mod_folder.startswith(\".\"):\n",
    "                continue\n",
    "            ssn_path = os.path.join(root_dir, mod_folder, 'ssn')\n",
    "            if not os.path.isdir(ssn_path):\n",
    "                continue\n",
    "            for snr in os.listdir(ssn_path):\n",
    "                if snr.startswith(\".\"):\n",
    "                    continue\n",
    "                snr_path = os.path.join(ssn_path, snr)\n",
    "                for audio_file in os.listdir(snr_path):\n",
    "                    audio_path = os.path.join(snr_path, audio_file)\n",
    "                    self.all_samples.append(audio_path)\n",
    "        idx = 0\n",
    "        val_list = []\n",
    "        for i in range(0, len(self.all_samples), 180):\n",
    "            val_list.extend(self.all_samples[i:i+36])\n",
    "        if self.state == 'train':\n",
    "            self.samples = [item for item in self.all_samples if item not in val_list]\n",
    "        elif self.state == 'valid':\n",
    "            self.samples = val_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.samples[idx]\n",
    "        split_str = audio_path.split('/')\n",
    "        output = [split_str[-4], split_str[-3], split_str[-2], split_str[-1].split('_')[-1].split('.')[0]]\n",
    "        numbers = int(''.join(re.findall(r'\\d+', output[0])))\n",
    "        noise_type = self.noise_types[output[1]]\n",
    "        snr = self.snrs[output[2]]\n",
    "        utt = int(output[3])\n",
    "        score = self.scores[numbers-1][noise_type][snr][utt-1]\n",
    "        \n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        waveform = torch.mean(waveform, dim=0)\n",
    "        \n",
    "        # Pad or trim the audio to 3 seconds\n",
    "        desired_length = sample_rate * 3  # keep 3 seconds\n",
    "        if waveform.size(-1) < desired_length:\n",
    "            padding = desired_length - waveform.size(-1)\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding), \"constant\")\n",
    "        elif waveform.size(-1) > desired_length:\n",
    "            waveform = waveform[..., :desired_length]\n",
    "        # if self.transform:\n",
    "        #     waveform = self.transform(waveform)\n",
    "\n",
    "        return waveform, waveform, score\n",
    "dataset_train = HurricaneData('train')\n",
    "dataset_valid = HurricaneData('valid')\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=3)\n",
    "for i, j in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train.__len__())\n",
    "print(dataset_valid.__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"1\", \"2\", \"3\"]\n",
    "c = [4,5,6]\n",
    "b =[]\n",
    "b=b+a\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    print(i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "data = scipy.io.loadmat('/home/ubuntu/elec823/cache.mat')['intell']\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = '*/123/123/hurricane/mod10/ssn/snrLo/hvd_009.wav'\n",
    "\n",
    "# 使用'/'分割字符串\n",
    "split_str = input_str.split('/')\n",
    "\n",
    "# 从分割后的字符串列表中提取所需部分\n",
    "output = [split_str[-4], split_str[-3], split_str[-2], split_str[-1].split('_')[-1].split('.')[0]]\n",
    "\n",
    "print(output)\n",
    "import re\n",
    "noise_types = {\"cs\":0, \"ssn\":1}\n",
    "snrs = {\"snrHi\":0, \"snrMid\":1, \"snrLo\":2}\n",
    "numbers = int(''.join(re.findall(r'\\d+', output[0])))\n",
    "noise_type = noise_types[output[1]]\n",
    "snr = snrs[output[2]]\n",
    "utt = int(output[3])\n",
    "print(numbers, noise_type, snr, utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import os\n",
    "from utils import *\n",
    "from models import *\n",
    "from my_loss import *\n",
    "from data_process import *\n",
    "\n",
    "asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(\n",
    "            \"nvidia/stt_en_conformer_transducer_xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmel = asr_model.preprocessor\n",
    "conformer_encoder = asr_model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioToMelSpectrogramPreprocessor(\n",
       "  (featurizer): FilterbankFeatures()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "logmel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "\n",
    "# create a random tensor of size [B, 10]\n",
    "x = torch.randn(8, 10)\n",
    "\n",
    "# unsqueeze the tensor along the last dimension to create a new dimension of size 151\n",
    "x = torch.unsqueeze(x, -1)\n",
    "x = x.repeat(1, 1, 151)  # repeat the tensor along the new dimension\n",
    "\n",
    "# print the size of the resulting tensor\n",
    "print(x.size())  # should output [B, 10, 151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randn(8, 512, 151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = torch.cat((x, y), dim=1)\n",
    "print(concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you need to be using OpenAI Python v0.27.0 for the code below to work\n",
    "import openai\n",
    "\n",
    "openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1,2,3])\n",
    "a.shape\n",
    "a.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
