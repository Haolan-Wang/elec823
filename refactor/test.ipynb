{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import os\n",
    "from utils import *\n",
    "from models import *\n",
    "from my_loss import *\n",
    "from data_process import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WordConfidence_CMP().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(\n",
    "            \"nvidia/stt_en_conformer_transducer_xlarge\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j  in asr_model.cfg.items():\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSTANTS = InitializationTrain(\n",
    "    verbose=True\n",
    ")\n",
    "dataset = CPCdataBinaural(metadata=CONSTANTS.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderPredictor().to(CONSTANTS.device)\n",
    "mel = model.logmel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset, batch_size=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listener Info (Audiogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listener_info = ListenerInfo(['L0231', 'L0201'])\n",
    "audiogram_l = [listener_info.info[i]['audiogram_l'] for i in range(len(listener_info.info))]\n",
    "audiogram_r = [listener_info.info[i]['audiogram_r'] for i in range(len(listener_info.info))]\n",
    "audiogram_cfs = [listener_info.info[i]['audiogram_cfs'] for i in range(len(listener_info.info))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiogram_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listener_info = ListenerInfo(['L0231', 'L0201'])\n",
    "print(listener_info.info)\n",
    "print(listener_info.info[0]['audiogram_l'])\n",
    "print(listener_info.info[0]['audiogram_r'])\n",
    "print(listener_info.info[0]['audiogram_cfs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for speech_input_l, speech_input_r, info_dict in tqdm(train_loader, desc=\"Training:\"):\n",
    "    mel_feature_l, mel_feature_length = mel(\n",
    "            input_signal=speech_input_l.to(device),\n",
    "            length=torch.full((speech_input_l.shape[0],), speech_input_l.shape[1]).to(device),\n",
    "        )\n",
    "    listener_info = ListenerInfo(info_dict['listener'])\n",
    "    listener_info.info['audiogram_l']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.asr_model.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item, value in model.asr_model.cfg.items():\n",
    "    print(item, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item, value in model.asr_model.cfg['preprocessor'].items():\n",
    "    print(item, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# 示例数据\n",
    "a = np.array([0, 1, 2, 3, 4, 5, 6, 7])  # 8元素的频率数组\n",
    "b = np.array([10, 20, 30, 40, 50, 60, 70, 80])  # 8元素的值数组\n",
    "c = np.linspace(0, 7, 80)  # 80元素的频率数组\n",
    "\n",
    "# 创建线性插值函数\n",
    "linear_interpolation = interp1d(a, b)\n",
    "\n",
    "# 计算c中每个频率对应的值\n",
    "result = linear_interpolation(c)\n",
    "\n",
    "# 打印结果\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mel_to_hz(mel):\n",
    "    return 700 * (10**(mel / 2595) - 1)\n",
    "\n",
    "def hz_to_mel(hz):\n",
    "    return 2595 * np.log10(1 + hz / 700)\n",
    "\n",
    "def get_central_frequencies(nfilt, lowfreq, highfreq):\n",
    "    low_mel = hz_to_mel(lowfreq)\n",
    "    high_mel = hz_to_mel(highfreq)\n",
    "\n",
    "    mel_points = np.linspace(low_mel, high_mel, nfilt + 2)  # nfilt + 2 points to include bounds\n",
    "    hz_points = mel_to_hz(mel_points)\n",
    "\n",
    "    central_frequencies = hz_points[1:-1]  # exclude the first and last points\n",
    "    return central_frequencies\n",
    "\n",
    "\n",
    "nfilt = 80\n",
    "lowfreq = 0\n",
    "highfreq = 8000\n",
    "\n",
    "central_frequencies = get_central_frequencies(nfilt, lowfreq, highfreq)\n",
    "print(central_frequencies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(central_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HurricaneData(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        for label_folder in os.listdir(root_dir):\n",
    "            label_path = os.path.join(root_dir, label_folder)\n",
    "            if os.path.isdir(label_path):\n",
    "                for audio_file in os.listdir(os.path.join(label_path,'ssn')):\n",
    "                    audio_path = os.path.join(label_path, audio_file)\n",
    "                    self.samples.append(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_dir = '/home/ubuntu/elec823/hurricane'\n",
    "print(os.listdir(root_dir))\n",
    "label_folder = os.listdir(root_dir)[0]\n",
    "label_path = os.path.join(root_dir, label_folder)\n",
    "print(label_path)\n",
    "print(os.path.join(label_path,'ssn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mod_folder in os.listdir(root_dir):\n",
    "    ssn_path = os.path.join(root_dir, mod_folder, 'ssn')\n",
    "print(os.listdir(ssn_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "root_dir = '/home/ubuntu/elec823/hurricane'\n",
    "samples = []\n",
    "for mod_folder in os.listdir(root_dir):\n",
    "    if mod_folder.startswith(\".\"):\n",
    "        continue\n",
    "    ssn_path = os.path.join(root_dir, mod_folder, 'ssn')\n",
    "    # print(ssn_path)\n",
    "    for snr in os.listdir(ssn_path):\n",
    "        if snr.startswith(\".\"):\n",
    "            continue\n",
    "        snr_path = os.path.join(ssn_path, snr)\n",
    "        for audio_file in os.listdir(snr_path):\n",
    "            audio_path = os.path.join(snr_path, audio_file)\n",
    "            samples.append(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(samples))\n",
    "print(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "waveform, sample_rate = torchaudio.load(samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform.shape\n",
    "import torch\n",
    "a = torch.mean(waveform, dim=0)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(label_path,'ssn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HurricaneData(Dataset):\n",
    "    def __init__(self, state, root_dir='/home/ubuntu/elec823/hurricane', transform=None):\n",
    "        self.state = state\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.scores = scipy.io.loadmat(os.path.join(root_dir, 'scores.mat'))['intell']\n",
    "        self.all_samples = []\n",
    "        self.noise_types = {\"cs\":0, \"ssn\":1}\n",
    "        self.snrs = {\"snrHi\":0, \"snrMid\":1, \"snrLo\":2}\n",
    "\n",
    "        for mod_folder in os.listdir(root_dir):\n",
    "            if mod_folder.startswith(\".\"):\n",
    "                continue\n",
    "            ssn_path = os.path.join(root_dir, mod_folder, 'ssn')\n",
    "            if not os.path.isdir(ssn_path):\n",
    "                continue\n",
    "            for snr in os.listdir(ssn_path):\n",
    "                if snr.startswith(\".\"):\n",
    "                    continue\n",
    "                snr_path = os.path.join(ssn_path, snr)\n",
    "                for audio_file in os.listdir(snr_path):\n",
    "                    audio_path = os.path.join(snr_path, audio_file)\n",
    "                    self.all_samples.append(audio_path)\n",
    "        idx = 0\n",
    "        val_list = []\n",
    "        for i in range(0, len(self.all_samples), 180):\n",
    "            val_list.extend(self.all_samples[i:i+36])\n",
    "        if self.state == 'train':\n",
    "            self.samples = [item for item in self.all_samples if item not in val_list]\n",
    "        elif self.state == 'valid':\n",
    "            self.samples = val_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.samples[idx]\n",
    "        split_str = audio_path.split('/')\n",
    "        output = [split_str[-4], split_str[-3], split_str[-2], split_str[-1].split('_')[-1].split('.')[0]]\n",
    "        numbers = int(''.join(re.findall(r'\\d+', output[0])))\n",
    "        noise_type = self.noise_types[output[1]]\n",
    "        snr = self.snrs[output[2]]\n",
    "        utt = int(output[3])\n",
    "        score = self.scores[numbers-1][noise_type][snr][utt-1]\n",
    "        \n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        waveform = torch.mean(waveform, dim=0)\n",
    "        \n",
    "        # Pad or trim the audio to 3 seconds\n",
    "        desired_length = sample_rate * 3  # keep 3 seconds\n",
    "        if waveform.size(-1) < desired_length:\n",
    "            padding = desired_length - waveform.size(-1)\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding), \"constant\")\n",
    "        elif waveform.size(-1) > desired_length:\n",
    "            waveform = waveform[..., :desired_length]\n",
    "        # if self.transform:\n",
    "        #     waveform = self.transform(waveform)\n",
    "\n",
    "        return waveform, waveform, score\n",
    "dataset_train = HurricaneData('train')\n",
    "dataset_valid = HurricaneData('valid')\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=3)\n",
    "for i, j in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train.__len__())\n",
    "print(dataset_valid.__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"1\", \"2\", \"3\"]\n",
    "c = [4,5,6]\n",
    "b =[]\n",
    "b=b+a\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    print(i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "data = scipy.io.loadmat('/home/ubuntu/elec823/cache.mat')['intell']\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = '*/123/123/hurricane/mod10/ssn/snrLo/hvd_009.wav'\n",
    "\n",
    "# 使用'/'分割字符串\n",
    "split_str = input_str.split('/')\n",
    "\n",
    "# 从分割后的字符串列表中提取所需部分\n",
    "output = [split_str[-4], split_str[-3], split_str[-2], split_str[-1].split('_')[-1].split('.')[0]]\n",
    "\n",
    "print(output)\n",
    "import re\n",
    "noise_types = {\"cs\":0, \"ssn\":1}\n",
    "snrs = {\"snrHi\":0, \"snrMid\":1, \"snrLo\":2}\n",
    "numbers = int(''.join(re.findall(r'\\d+', output[0])))\n",
    "noise_type = noise_types[output[1]]\n",
    "snr = snrs[output[2]]\n",
    "utt = int(output[3])\n",
    "print(numbers, noise_type, snr, utt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import os\n",
    "from utils import *\n",
    "from models import *\n",
    "from my_loss import *\n",
    "from data_process import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmel = asr_model.preprocessor\n",
    "conformer_encoder = asr_model.encoder\n",
    "asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(\n",
    "            \"nvidia/stt_en_conformer_transducer_xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderPredictor_Fusion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.6621e-03,  9.7242e-05,  1.2429e-04,  ..., -2.3975e-03,\n",
      "        -1.4652e-04, -3.4295e-04], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(model.predictor[0].weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "\n",
    "# create a random tensor of size [B, 10]\n",
    "x = torch.randn(8, 10)\n",
    "\n",
    "# unsqueeze the tensor along the last dimension to create a new dimension of size 151\n",
    "x = torch.unsqueeze(x, -1)\n",
    "x = x.repeat(1, 1, 151)  # repeat the tensor along the new dimension\n",
    "\n",
    "# print the size of the resulting tensor\n",
    "print(x.size())  # should output [B, 10, 151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randn(8, 512, 151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = torch.cat((x, y), dim=1)\n",
    "print(concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([1,2,3])\n",
    "a.shape\n",
    "a.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 示例：不等长的张量列表\n",
    "tensor_list = [\n",
    "    torch.tensor([1, 2, 3]),\n",
    "    torch.tensor([4, 5]),\n",
    "    torch.tensor([6, 7, 8, 9])\n",
    "]\n",
    "\n",
    "# 使用pad_sequence将列表转换为固定长度的张量\n",
    "padded_tensor = pad_sequence(tensor_list, batch_first=True, padding_value=0)\n",
    "\n",
    "print(\"Padded tensor:\")\n",
    "print(padded_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Test, self).__init__()\n",
    "        self.threshold = nn.Parameter(torch.tensor(0.5), requires_grad=True)\n",
    "\n",
    "    def forward(self, word_confidence, valid_len):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word_confidence (torch.Tensor): [Batch, 10]\n",
    "            valid_len (torch.Tensor): [Batch, 1]\n",
    "        \"\"\"\n",
    "        # greater_than_thr = (word_confidence > self.threshold).sum(dim=1, keepdim=True).float().requires_grad_()\n",
    "        # print(\"great: \", greater_than_thr)\n",
    "        # output = torch.div(greater_than_thr, valid_len.view(-1, 1))\n",
    "        output = word_confidence * self.threshold\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0.2,0.5,0.8], requires_grad=True, dtype=torch.float32)\n",
    "b = torch.tensor([0.1,0.9,0.4], requires_grad=True, dtype=torch.float32)\n",
    "c = torch.tensor([7,8,9], requires_grad=True, dtype=torch.float32)\n",
    "loss = torch.nn.MSELoss()\n",
    "test = Test()\n",
    "\n",
    "x = torch.stack([a,b], dim=0)\n",
    "print(x)\n",
    "print(x.shape)\n",
    "# y = a+b\n",
    "y = test(x, torch.tensor([3,3]))\n",
    "print(y)\n",
    "t = torch.tensor([[0.5, 0.5, 0.5], [0.5, 0.5, 0.5]])\n",
    "loss = loss(y, t)\n",
    "loss.backward()\n",
    "\n",
    "print(loss.retain_grad())\n",
    "print(x.grad)\n",
    "tensor_threshold = test.threshold.detach()\n",
    "\n",
    "print(tensor_threshold.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([5,2,3], requires_grad=True, dtype=torch.float32)\n",
    "b = torch.tensor([4,5,6], requires_grad=True, dtype=torch.float32)\n",
    "c = torch.tensor([7,8,9], requires_grad=True, dtype=torch.float32)\n",
    "loss = torch.nn.MSELoss()\n",
    "test = Test()\n",
    "# y = a+b\n",
    "y = test(a)\n",
    "print(y)\n",
    "y = torch.max(a,b)\n",
    "print(y)\n",
    "loss = loss(y,c)\n",
    "loss.backward()\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
