{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-04-23 04:34:33 optimizers:54] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
      "[NeMo W 2023-04-23 04:34:33 nemo_logging:349] /home/ubuntu/.local/lib/python3.8/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n",
      "[NeMo W 2023-04-23 04:34:34 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import os\n",
    "from utils import *\n",
    "from models import *\n",
    "from my_loss import *\n",
    "from data_process import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Tesla T4\n",
      "Model Name:  None\n",
      "Checking paths...\n",
      "Warning: Model name is not specified. It's EXPECTED if you are initializing a 'models.py'. Otherwise Your model will be saved in 'checkpoints/00_TMP_MODEL' folder.\n",
      "Folder already exists: /home/ubuntu/elec823/log/00_TMP_MODEL\n",
      "Folder already exists: /home/ubuntu/elec823/checkpoints/00_TMP_MODEL\n",
      "Folder already exists: /home/ubuntu/elec823/log/last_output/\n",
      "Seed set to: 3407\n"
     ]
    }
   ],
   "source": [
    "CONSTANTS = InitializationTrain(\n",
    "    verbose=True\n",
    ")\n",
    "dataset = CPCdataBinaural(metadata=CONSTANTS.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-04-23 04:34:40 mixins:170] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-04-23 04:34:40 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /data/NeMo_ASR_SET/English/v2.0/train/tarred_audio_manifest.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20.0\n",
      "    min_duration: 0.1\n",
      "    shuffle_n: 2048\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths: /data/NeMo_ASR_SET/English/v2.0/train/audio__OP_0..4095_CL_.tar\n",
      "    \n",
      "[NeMo W 2023-04-23 04:34:40 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n",
      "[NeMo W 2023-04-23 04:34:40 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath:\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
      "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: na\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-04-23 04:34:40 features:287] PADDING: 0\n",
      "[NeMo I 2023-04-23 04:34:43 save_restore_connector:247] Model EncDecCTCModelBPE was successfully restored from /home/ubuntu/.cache/huggingface/hub/models--nvidia--stt_en_conformer_ctc_large/snapshots/2c8326e4e43ae5b994612cfea3f3029818fb23c6/stt_en_conformer_ctc_large.nemo.\n"
     ]
    }
   ],
   "source": [
    "model = EncoderPredictor().to(CONSTANTS.device)\n",
    "mel = model.logmel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=dataset, batch_size=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listener Info (Audiogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "listener_info = ListenerInfo(['L0231', 'L0201'])\n",
    "audiogram_l = [listener_info.info[i]['audiogram_l'] for i in range(len(listener_info.info))]\n",
    "audiogram_r = [listener_info.info[i]['audiogram_r'] for i in range(len(listener_info.info))]\n",
    "audiogram_cfs = [listener_info.info[i]['audiogram_cfs'] for i in range(len(listener_info.info))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 20, 20, 50, 65, 60, 75, 85], [10, 10, 20, 60, 85, 80, 85, 75]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audiogram_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OrderedDict([('audiogram_l', [20, 20, 20, 50, 65, 60, 75, 85]), ('audiogram_r', [20, 15, 10, 30, 50, 50, 55, 70]), ('audiogram_cfs', [250, 500, 1000, 2000, 3000, 4000, 6000, 8000])]), OrderedDict([('audiogram_l', [10, 10, 20, 60, 85, 80, 85, 75]), ('audiogram_r', [5, 5, 10, 30, 40, 60, 85, 85]), ('audiogram_cfs', [250, 500, 1000, 2000, 3000, 4000, 6000, 8000])])]\n",
      "[20, 20, 20, 50, 65, 60, 75, 85]\n",
      "[20, 15, 10, 30, 50, 50, 55, 70]\n",
      "[250, 500, 1000, 2000, 3000, 4000, 6000, 8000]\n"
     ]
    }
   ],
   "source": [
    "listener_info = ListenerInfo(['L0231', 'L0201'])\n",
    "print(listener_info.info)\n",
    "print(listener_info.info[0]['audiogram_l'])\n",
    "print(listener_info.info[0]['audiogram_r'])\n",
    "print(listener_info.info[0]['audiogram_cfs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training::   0%|          | 0/1621 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/elec823/refactor/test.ipynb Cell 7\u001b[0m in \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225434227d/home/ubuntu/elec823/refactor/test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m mel_feature_l, mel_feature_length \u001b[39m=\u001b[39m mel(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225434227d/home/ubuntu/elec823/refactor/test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         input_signal\u001b[39m=\u001b[39mspeech_input_l\u001b[39m.\u001b[39mto(device),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225434227d/home/ubuntu/elec823/refactor/test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         length\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfull((speech_input_l\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],), speech_input_l\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mto(device),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225434227d/home/ubuntu/elec823/refactor/test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225434227d/home/ubuntu/elec823/refactor/test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m listener_info \u001b[39m=\u001b[39m ListenerInfo(info_dict[\u001b[39m'\u001b[39m\u001b[39mlistener\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225434227d/home/ubuntu/elec823/refactor/test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m listener_info\u001b[39m.\u001b[39;49minfo[\u001b[39m'\u001b[39;49m\u001b[39maudiogram_l\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225434227d/home/ubuntu/elec823/refactor/test.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "for speech_input_l, speech_input_r, info_dict in tqdm(train_loader, desc=\"Training:\"):\n",
    "    mel_feature_l, mel_feature_length = mel(\n",
    "            input_signal=speech_input_l.to(device),\n",
    "            length=torch.full((speech_input_l.shape[0],), speech_input_l.shape[1]).to(device),\n",
    "        )\n",
    "    listener_info = ListenerInfo(info_dict['listener'])\n",
    "    listener_info.info['audiogram_l']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.asr_model.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item, value in model.asr_model.cfg.items():\n",
    "    print(item, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item, value in model.asr_model.cfg['preprocessor'].items():\n",
    "    print(item, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.         10.88607595 11.7721519  12.65822785 13.5443038  14.43037975\n",
      " 15.3164557  16.20253165 17.08860759 17.97468354 18.86075949 19.74683544\n",
      " 20.63291139 21.51898734 22.40506329 23.29113924 24.17721519 25.06329114\n",
      " 25.94936709 26.83544304 27.72151899 28.60759494 29.49367089 30.37974684\n",
      " 31.26582278 32.15189873 33.03797468 33.92405063 34.81012658 35.69620253\n",
      " 36.58227848 37.46835443 38.35443038 39.24050633 40.12658228 41.01265823\n",
      " 41.89873418 42.78481013 43.67088608 44.55696203 45.44303797 46.32911392\n",
      " 47.21518987 48.10126582 48.98734177 49.87341772 50.75949367 51.64556962\n",
      " 52.53164557 53.41772152 54.30379747 55.18987342 56.07594937 56.96202532\n",
      " 57.84810127 58.73417722 59.62025316 60.50632911 61.39240506 62.27848101\n",
      " 63.16455696 64.05063291 64.93670886 65.82278481 66.70886076 67.59493671\n",
      " 68.48101266 69.36708861 70.25316456 71.13924051 72.02531646 72.91139241\n",
      " 73.79746835 74.6835443  75.56962025 76.4556962  77.34177215 78.2278481\n",
      " 79.11392405 80.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# 示例数据\n",
    "a = np.array([0, 1, 2, 3, 4, 5, 6, 7])  # 8元素的频率数组\n",
    "b = np.array([10, 20, 30, 40, 50, 60, 70, 80])  # 8元素的值数组\n",
    "c = np.linspace(0, 7, 80)  # 80元素的频率数组\n",
    "\n",
    "# 创建线性插值函数\n",
    "linear_interpolation = interp1d(a, b)\n",
    "\n",
    "# 计算c中每个频率对应的值\n",
    "result = linear_interpolation(c)\n",
    "\n",
    "# 打印结果\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  22.12006573   44.93912761   68.47927399   92.7632912   117.81468564\n",
      "  143.6577065   170.31736924  197.81947982  226.19065969  255.4583715\n",
      "  285.65094575  316.79760819  348.92850808  382.07474742  416.26841104\n",
      "  451.54259764  487.93145185  525.47019727  564.19517057  604.14385667\n",
      "  645.35492499  687.86826694  731.72503449  776.96768002  823.63999739\n",
      "  871.78716437  921.45578634  972.69394144 1025.55122705 1080.07880785\n",
      " 1136.32946531 1194.35764884 1254.21952842 1315.97304901 1379.67798665\n",
      " 1445.3960063  1513.19072154 1583.12775615 1655.27480762 1729.70171269\n",
      " 1806.48051495 1885.68553457 1967.39344025 2051.68332342 2138.63677481\n",
      " 2228.33796343 2320.87371803 2416.33361116 2514.81004589 2616.39834519\n",
      " 2721.1968443  2829.30698581 2940.83341795 3055.88409582 3174.5703859\n",
      " 3297.0071739  3423.3129759  3553.6100531  3688.02453017 3826.68651734\n",
      " 3969.73023633 4117.29415026 4269.52109773 4426.55843104 4588.55815882\n",
      " 4755.67709321 4928.07700162 5105.92476332 5289.39253099 5478.65789735\n",
      " 5673.90406706 5875.32003406 6083.10076453 6297.44738559 6518.56738\n",
      " 6746.674787   6981.99040948 7224.74202773 7475.1646199  7733.5005895 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mel_to_hz(mel):\n",
    "    return 700 * (10**(mel / 2595) - 1)\n",
    "\n",
    "def hz_to_mel(hz):\n",
    "    return 2595 * np.log10(1 + hz / 700)\n",
    "\n",
    "def get_central_frequencies(nfilt, lowfreq, highfreq):\n",
    "    low_mel = hz_to_mel(lowfreq)\n",
    "    high_mel = hz_to_mel(highfreq)\n",
    "\n",
    "    mel_points = np.linspace(low_mel, high_mel, nfilt + 2)  # nfilt + 2 points to include bounds\n",
    "    hz_points = mel_to_hz(mel_points)\n",
    "\n",
    "    central_frequencies = hz_points[1:-1]  # exclude the first and last points\n",
    "    return central_frequencies\n",
    "\n",
    "\n",
    "nfilt = 80\n",
    "lowfreq = 0\n",
    "highfreq = 8000\n",
    "\n",
    "central_frequencies = get_central_frequencies(nfilt, lowfreq, highfreq)\n",
    "print(central_frequencies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(central_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
